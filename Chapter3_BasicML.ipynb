{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Introduction\n",
    "This notebook performs some basic Machine Learning analysis of the New York Rolling Sales data from the [Doing Data Science : Straight Talk from the Frontline book][booklink] by Cathy O'Neil & Rachel Schutt published by O'Reilly Media. The data can be downloaded [here][datalink] and specifically I will be briefly looking at the `rollingsales_manhattan.xls` data.\n",
    "\n",
    "The file contains housing sales data for Manhattan properties, including the neighbourhood, building class (family home, condo, etc), sale price, sale date, etc. \n",
    "\n",
    "I will be doing the data analysis using Julia.\n",
    "\n",
    "[booklink]: https://www.oreilly.com/library/view/doing-data-science/9781449363871/\n",
    "[datalink]: https://github.com/oreillymedia/doing_data_science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Data Cleaning\n",
    "I'll be performing some data cleaning steps that I previously performed in another notebook. These have been detailed [here:](https://mybinder.org/v2/gh/ncalvertuk/DoingDataScienceNbs_Julia/master?filepath=Chapter2_RollingSales.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:37:27.143Z",
     "iopub.status.busy": "2020-06-23T16:37:27.128Z",
     "iopub.status.idle": "2020-06-23T16:37:45.437Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, Gadfly, Queryverse, Statistics, Dates,Plots,StatsPlots,VegaDatasets,CSV, JSON\n",
    "plotly()\n",
    "Plots.PlotlyBackend()\n",
    "D = DataFrame(load(\"./rollingsales_manhattan.xls\", \"Manhattan!A5:U27400\"));\n",
    "# Clean up the property names\n",
    "for pn in propertynames(D)\n",
    "  rename!(D,pn => Symbol(filter(x -> !isspace(x),string(pn))))\n",
    "  rename!(D,pn => Symbol(replace(string(pn),\"-\"=> \"\")))\n",
    "end\n",
    "# Make the strings lower case.\n",
    "D = D |> @mutate(NEIGHBORHOOD = lowercase(join(split(strip(_.NEIGHBORHOOD)))),BUILDINGCLASSCATEGORY = lowercase(join(split(strip(_.BUILDINGCLASSCATEGORY)))),ADDRESS = lowercase(_.ADDRESS),EASEMENT = lowercase(join(split(strip(_.EASEMENT)))),BUILDINGCLASSATPRESENT = lowercase(join(split(strip(_.BUILDINGCLASSATPRESENT)))),BUILDINGCLASSATTIMEOFSALE = lowercase(join(split(strip(_.BUILDINGCLASSATTIMEOFSALE))))) |> DataFrame\n",
    "# Keep family homes with a sale price of more than $100k and with a gross square foot >0\n",
    "D_fam = D |> @filter(_.SALEPRICE > 100000 && _.GROSSSQUAREFEET >0) |> @mutate(PRICEPERSQF = _.SALEPRICE/_.GROSSSQUAREFEET) |> @mutate(FAMILYHOME = occursin(\"family\",_.BUILDINGCLASSCATEGORY)) |> @filter(_.FAMILYHOME == true)|> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now that we have the data, lets perform some linear regression. We'll once again start with a basic (and inaccurate!) model before increasing the number of features. Let's start with a single factor - the gross square foot.\n",
    "\n",
    "First let's have a look at the data by histogramming the two variables and generating a scatter plot. We'll use a logarithmic scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:33:19.964Z",
     "iopub.status.busy": "2020-06-23T16:33:01.446Z",
     "iopub.status.idle": "2020-06-23T16:33:34.222Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#p1 = D_fam |> @vlplot(:bar, x={:GROSSSQUAREFEET, bin=true,scale={type=\"log\",base=10}}, y=\"count()\")\n",
    "p1 = D_fam |> @vlplot(\n",
    "    :bar,\n",
    "    transform=[\n",
    "        {calculate=\"log(datum.GROSSSQUAREFEET)/log(10)\", as=\"log_x\"},\n",
    "        {field=\"log_x\",bin={\"maxbins\"=30},as=\"bin_log_x\"},\n",
    "        {calculate=\"pow(10, datum.bin_log_x)\", as=\"x1\"},\n",
    "        {calculate=\"pow(10, datum.bin_log_x_end)\", as=\"x2\"}\n",
    "    ],\n",
    "    x={\"x1:q\", scale={type=\"log\",base=10},axis={tickCount=5},title=\"Gross Square Feet\"},\n",
    "    x2=:x2,\n",
    "    y={aggregate=\"count\",type=\"quantitative\"}\n",
    ")\n",
    "p2 = D_fam |> @vlplot(\n",
    "    :bar,\n",
    "    transform=[\n",
    "        {calculate=\"log(datum.SALEPRICE)/log(10)\", as=\"log_x\"},\n",
    "        {field=\"log_x\",bin={\"maxbins\"=30},as=\"bin_log_x\"},\n",
    "        {calculate=\"pow(10, datum.bin_log_x)\", as=\"x1\"},\n",
    "        {calculate=\"pow(10, datum.bin_log_x_end)\", as=\"x2\"}\n",
    "    ],\n",
    "    x={\"x1:q\", scale={type=\"log\",base=10},axis={tickCount=5},title=\"Sale Price (USD)\"},\n",
    "    x2=:x2,\n",
    "    y={aggregate=\"count\",type=\"quantitative\"}\n",
    ")\n",
    "p3 = D_fam |> @vlplot(:circle,x={:GROSSSQUAREFEET,scale={type=\"log\",base=10}},y={:SALEPRICE,scale={type=\"log\",base=10}})\n",
    "display(p1)\n",
    "display(p2)\n",
    "display(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "It is clear from the plots that there are a number of factors to consider here when trying to predict the sale price and just using the gross square footage will not lead to a satisfactory fit. In particular there distributions appear to be bimodal, particularly the sale price. Lets go ahead anyway!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Linear Regression using MLJ\n",
    "I'll be using [MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/) to perform the basic Machine Learning here. I first create a new DataFrame containing the variables I'm interested in and then use the unpack method to separate this out into X & y arrays. MLJ requires the input variables to be in a table format, I found I had to reshape X when it contained only a single variable.\n",
    "\n",
    "After getting the data in the correct format I load in the model I'm interested in - in this case I'm using Linear Regression. I then instantiate the model, setting the intercept to 0 before training the model and predicting on my test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:33:34.266Z",
     "iopub.status.busy": "2020-06-23T16:33:34.249Z",
     "iopub.status.idle": "2020-06-23T16:34:08.240Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using MLJ,MLJLinearModels\n",
    "D_fam_sub = D_fam |> @map({X = log(_.GROSSSQUAREFEET),Y = log(_.SALEPRICE)}) |> DataFrame\n",
    "y,X = unpack(D_fam_sub,==(:Y),==(:X);:X=>Continuous,:Y=>Continuous)\n",
    "X = MLJ.table(reshape(X,291,1))\n",
    "@load LinearRegressor pkg=\"MLJLinearModels\"\n",
    "model = LinearRegressor(fit_intercept = false)\n",
    "LR = machine(model, X, y)\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split\n",
    "fit!(LR, rows=train)\n",
    "yhat = predict(LR, rows=test);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "To evaluate the model I can use the evaluate function, notice that I call evaluate! as I'm applying it to an existing model. I also define rsq as a new evaluation metric, which is then used in the evaluate! function. The evaluate function uses cross-validation as default, with 6 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:09.820Z",
     "iopub.status.busy": "2020-06-23T16:34:08.258Z",
     "iopub.status.idle": "2020-06-23T16:34:18.187Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "rsq(y_hat,y) = 1 - sum((y .- y_hat).^2)/sum((y.-mean(y)).^2)\n",
    "evaluate!(LR,measure=[l2,rms,rmslp1,rsq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "As expected, the model is pretty useless! Let's get the fitted parameters, generate a line from these and then add this to a plot along with the original data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:18.219Z",
     "iopub.status.busy": "2020-06-23T16:34:18.206Z",
     "iopub.status.idle": "2020-06-23T16:34:40.035Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "x1 = range(0,stop=9.5,length=1000)\n",
    "y1 = x1.*fitted_params(LR).coefs[1][2]\n",
    "\n",
    "p = Plots.plot(exp.(reshape(D_fam_sub[!,1],291,1)),exp.(reshape(D_fam_sub[!,2],291,1)), seriestype = :scatter,size = (400,400),xlabel=\"GROSSSQUAREFEET (sqf)\", ylabel = \"SALEPRICE (USD)\",title=\"Single Variable Fit\")\n",
    "Plots.plot!(p,exp.(x1),exp.(y1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We can also histogram the residuals, unlike in the last plot we won't transform them by taking the exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:40.081Z",
     "iopub.status.busy": "2020-06-23T16:34:40.056Z",
     "iopub.status.idle": "2020-06-23T16:34:46.325Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using KernelDensity,NumericalIntegration\n",
    "U = kde(y[test]-yhat)\n",
    "p1 = histogram(y[test]-yhat,bins= 30,title =\"Residual Histogram\",size = (300,200))\n",
    "mult = (0.2*87)./integrate(U.x,U.density) # histo bins are 0.2 wide, so we need to scale the density to match the plot\n",
    "Plots.plot!(p1,U.x,mult .*U.density)\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Pretty naff, let's see if we can improve this by increasing the number of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:46.372Z",
     "iopub.status.busy": "2020-06-23T16:34:46.350Z",
     "iopub.status.idle": "2020-06-23T16:34:51.206Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_fam_sub = D_fam |> @map({X = log(_.GROSSSQUAREFEET),X1 = log(_.LANDSQUAREFEET),Y = log(_.SALEPRICE)}) |> DataFrame\n",
    "y,X = unpack(D_fam_sub,==(:Y),!=(:Y));\n",
    "LR1 = machine(model, X, y)\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split\n",
    "fit!(LR1, rows=train)\n",
    "yhat1 = predict(LR1, rows=test);\n",
    "evaluate!(LR1,measure=[l2,rms,rmslp1,rsq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:51.234Z",
     "iopub.status.busy": "2020-06-23T16:34:51.223Z",
     "iopub.status.idle": "2020-06-23T16:34:51.379Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "U1 = kde(y[test]-yhat1)\n",
    "p2 = histogram(y[test]-yhat1,bins= 30,title =\"Residual Histogram\",size = (300,200))\n",
    "mult = (0.2*87)./integrate(U1.x,U1.density) # histo bins are 0.2 wide, so we need to scale the density to match the plot\n",
    "Plots.plot!(p2,U.x,mult .*U1.density)\n",
    "display(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Still a bit naff, not to be unexpected given that the price varies between neighborhoods as we've seen previously. Let's include this as a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:51.422Z",
     "iopub.status.busy": "2020-06-23T16:34:51.405Z",
     "iopub.status.idle": "2020-06-23T16:34:53.495Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_fam_sub = D_fam |> @map({X = log(_.GROSSSQUAREFEET),X1 = log(_.LANDSQUAREFEET),X2 = _.NEIGHBORHOOD,Y = log(_.SALEPRICE)}) |> DataFrame\n",
    "y,X = unpack(D_fam_sub,==(:Y),!=(:Y))\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We can see that the Neighbourhood has a ```Textual``` type, we want this to be a ```Multiclass``` type (with no ordering). We use ```coerce``` to force this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:53.535Z",
     "iopub.status.busy": "2020-06-23T16:34:53.515Z",
     "iopub.status.idle": "2020-06-23T16:34:55.253Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "coerce!(X,:X2=>Multiclass)\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now we implement the One Hot Encoder to convert from categorical variables to continuous. We create a new variable for each neighborhood, then each data row contains a 1 in the variable relating to the neighborhood and 0 in the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:55.281Z",
     "iopub.status.busy": "2020-06-23T16:34:55.269Z",
     "iopub.status.idle": "2020-06-23T16:34:58.656Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "hot = OneHotEncoder(ordered_factor=false);\n",
    "mach = fit!(machine(hot, X))\n",
    "X = transform(mach, X)\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Lets take a look at the first data row. As we can see, X2__alphabetcity = 1 and the other values are all 0 (Except Gross Square Feet and Land Square Feet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:34:58.701Z",
     "iopub.status.busy": "2020-06-23T16:34:58.684Z",
     "iopub.status.idle": "2020-06-23T16:35:02.218Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "first(X,1) |> pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:02.250Z",
     "iopub.status.busy": "2020-06-23T16:35:02.240Z",
     "iopub.status.idle": "2020-06-23T16:35:02.310Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "LR2 = machine(model, X, y)\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split\n",
    "fit!(LR2, rows=train)\n",
    "yhat2 = predict(LR2, rows=test);\n",
    "evaluate!(LR2,measure=[l2,rms,rmslp1,rsq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:02.340Z",
     "iopub.status.busy": "2020-06-23T16:35:02.329Z",
     "iopub.status.idle": "2020-06-23T16:35:02.606Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "U2 = kde(y[test]-yhat2)\n",
    "p3 = histogram(y[test]-yhat2,bins= range(-2,stop=10,length=51),title =\"Residual Histogram\",size = (300,200))\n",
    "mult = (0.24*87)./integrate(U2.x,U2.density) # histo bins are 0.2 wide, so we need to scale the density to match the plot\n",
    "Plots.plot!(p3,U2.x,mult .*U2.density)\n",
    "display(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:02.641Z",
     "iopub.status.busy": "2020-06-23T16:35:02.627Z",
     "iopub.status.idle": "2020-06-23T16:35:04.019Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "all_resids = [y[test]-yhat, y[test]-yhat1,y[test]-yhat2]\n",
    "p4 = histogram(all_resids,bins= range(-2,stop=10,length=51),title =\"Residual Histogram\",size = (300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "From the plot above it appears that the residuals are generally decreased when using the neighborhood however there are some outliers with large residuals. We can also see from the Cross-validation that the MSE varies wildly from 0.4 up to 60(!). This is most likely caused by the low numbers of properties in certain neighborhoods. We can see this in the table below.\n",
    "\n",
    "A solution to this could be to combine different neighbourhoods into larger geographical regions although this would have to be done carefully in case there were large differences in the average price per sqf in two neighbouring neighbourhoods, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:04.053Z",
     "iopub.status.busy": "2020-06-23T16:35:04.042Z",
     "iopub.status.idle": "2020-06-23T16:35:05.305Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "println(D_fam |>\n",
    "    @groupby(_.NEIGHBORHOOD) |>\n",
    "    @map({Key=key(_), Count=length(_)}) |>\n",
    "    DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Finally, we'll add in one extra variable - Building class. This is another categorical variable, so we'll use One Hot Encoding once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:05.335Z",
     "iopub.status.busy": "2020-06-23T16:35:05.323Z",
     "iopub.status.idle": "2020-06-23T16:35:07.361Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_fam_sub = D_fam |> @map({X = log(_.GROSSSQUAREFEET),X1 = log(_.LANDSQUAREFEET),X2 = _.NEIGHBORHOOD,X3 = _.BUILDINGCLASSCATEGORY,Y = log(_.SALEPRICE)}) |> DataFrame\n",
    "y,X = unpack(D_fam_sub,==(:Y),!=(:Y))\n",
    "coerce!(X,:X2=>Multiclass)\n",
    "coerce!(X,:X3=>Multiclass)\n",
    "mach = fit!(machine(hot, X))\n",
    "X = transform(mach, X)\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:07.426Z",
     "iopub.status.busy": "2020-06-23T16:35:07.406Z",
     "iopub.status.idle": "2020-06-23T16:35:07.597Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "LR3 = machine(model, X, y)\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split\n",
    "fit!(LR3, rows=train)\n",
    "yhat3 = predict(LR3, rows=test);\n",
    "evaluate!(LR3,measure=[l2,rms,rmslp1,rsq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:07.630Z",
     "iopub.status.busy": "2020-06-23T16:35:07.618Z",
     "iopub.status.idle": "2020-06-23T16:35:07.806Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "U3 = kde(y[test]-yhat3)\n",
    "p4 = histogram(y[test]-yhat3,bins= range(-2,stop=10,length=51),title =\"Residual Histogram\",size = (300,200))\n",
    "mult = (0.24*87)./integrate(U3.x,U3.density) # histo bins are 0.2 wide, so we need to scale the density to match the plot\n",
    "Plots.plot!(p4,U3.x,mult .*U3.density)\n",
    "display(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:07.856Z",
     "iopub.status.busy": "2020-06-23T16:35:07.842Z",
     "iopub.status.idle": "2020-06-23T16:35:08.326Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "all_resids = [y[test]-yhat, y[test]-yhat1,y[test]-yhat2,y[test]-yhat3]\n",
    "p4 = histogram(all_resids,bins= range(-2,stop=10,length=51),title =\"Residual Histogram\",size = (300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "This appears to have improved things somewhat compared to the previous fit. We can also interact the last two variables. Let's try that before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:08.360Z",
     "iopub.status.busy": "2020-06-23T16:35:08.348Z",
     "iopub.status.idle": "2020-06-23T16:35:08.844Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_fam_sub = D_fam |> @map({X = log(_.GROSSSQUAREFEET),X1 = log(_.LANDSQUAREFEET),X2 = string(_.NEIGHBORHOOD,_.BUILDINGCLASSCATEGORY),Y = log(_.SALEPRICE)}) |> DataFrame\n",
    "y,X = unpack(D_fam_sub,==(:Y),!=(:Y))\n",
    "coerce!(X,:X2=>Multiclass)\n",
    "mach = fit!(machine(hot, X))\n",
    "X = transform(mach, X)\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:08.893Z",
     "iopub.status.busy": "2020-06-23T16:35:08.878Z",
     "iopub.status.idle": "2020-06-23T16:35:09.118Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "LR4 = machine(model, X, y)\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split\n",
    "fit!(LR4, rows=train)\n",
    "yhat4 = predict(LR4, rows=test);\n",
    "evaluate!(LR4,measure=[l2,rms,rmslp1,rsq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:09.167Z",
     "iopub.status.busy": "2020-06-23T16:35:09.149Z",
     "iopub.status.idle": "2020-06-23T16:35:09.341Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "U4 = kde(y[test]-yhat4)\n",
    "p5 = histogram(y[test]-yhat4,bins= range(-2,stop=10,length=51),title =\"Residual Histogram\",size = (300,200))\n",
    "mult = (0.24*87)./integrate(U4.x,U4.density) # histo bins are 0.2 wide, so we need to scale the density to match the plot\n",
    "Plots.plot!(p5,U4.x,mult .*U4.density)\n",
    "display(p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:35:09.372Z",
     "iopub.status.busy": "2020-06-23T16:35:09.359Z",
     "iopub.status.idle": "2020-06-23T16:35:09.805Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "all_resids = [y[test]-yhat, y[test]-yhat1,y[test]-yhat2,y[test]-yhat3,y[test]-yhat4]\n",
    "p6 = histogram(all_resids,bins= range(-2,stop=10,length=51),title =\"Residual Histogram\",size = (300,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "This leads to more extreme residual values than the previous model. Again, by aggregating the neighbourhoods we may be able to overcome this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Classifying Neighbourhood using Latitude & Longitude\n",
    "The second part of the exercise was to predict the neighbourhood of properties, implementing a k-nearest neighbours classifier based on the latitude and longitude. This should be relatively straightforward but will require some data cleaning before getting started.\n",
    "\n",
    "Firstly we'll only keep properties that were sold with a price of > $10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-23T16:37:52.594Z",
     "iopub.status.busy": "2020-06-23T16:37:52.581Z",
     "iopub.status.idle": "2020-06-23T16:37:52.686Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_sub = D |> @filter(_.SALEPRICE > 10000 && _.GROSSSQUAREFEET >0)|> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Lets take a look at the addresses to see if there's any problems. As we can see there is a rather large space at the end of them, so we need to get rid of that. There's also some extra spaces before the word 'street' in some cases. Lets also add in the city and state (both New York in this case), the zip code, and the country (US)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-24T11:19:02.610Z",
     "iopub.status.busy": "2020-06-24T11:19:02.589Z",
     "iopub.status.idle": "2020-06-24T11:19:02.639Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_sub[!,:ADDRESS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-24T12:13:00.163Z",
     "iopub.status.busy": "2020-06-24T12:13:00.143Z",
     "iopub.status.idle": "2020-06-24T12:13:03.243Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_sub = D_sub |> @mutate(LONGADDRESS  = replace.(_.ADDRESS,\"  \" => \"\")) |> @mutate(LONGADDRESS  = string(_.LONGADDRESS,\", New York, NY, \", Int(_.ZIPCODE), \" US\")) |> @mutate(LONGADDRESS  = replace.(_.LONGADDRESS,\" , \" => \", \")) |> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We'll create a new DataFrame solely containing the unique addresses and neighborhoods. We'll extend this with the lats & longs when we've got them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-24T12:27:05.352Z",
     "iopub.status.busy": "2020-06-24T12:27:05.332Z",
     "iopub.status.idle": "2020-06-24T12:27:05.636Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_adds = D_sub |> @groupby(_.LONGADDRESS) |>\n",
    "    @map({LONGADDRESS=key(_), NEIGHBORHOOD=first(unique(_.NEIGHBORHOOD))}) |>\n",
    "    DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "To get the lats & longs I'll use [LocationIQ](https://locationiq.com/). You'll need to grab a free API key to use this, which limits the number of requests to 60 per minute. For our 1450 unique addresses this is going to take 24-25 minutes in total. We'll need to use the [HTTP.jl](https://github.com/JuliaWeb/HTTP.jl) package to send the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-24T13:52:36.115Z",
     "iopub.status.busy": "2020-06-24T13:52:36.091Z",
     "iopub.status.idle": "2020-06-24T14:16:24.178Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using ProgressMeter\n",
    "using HTTP\n",
    "using JSON\n",
    "lats =[]\n",
    "lons = []\n",
    "apikey = \"YOUR_API_KEY\"\n",
    "@showprogress for add in D_adds[!,:LONGADDRESS]\n",
    "  Add = replace.(replace.(add,\",\"=>\"\"),\" \"=>\"%20\")\n",
    "  url1 = string(\"https://us1.locationiq.com/v1/search.php?key=\",apikey,\"&q=\",Add,\"&format=json\");\n",
    "  try \n",
    "    r = HTTP.request(\"GET\", url1)\n",
    "  bod = JSON.parse(String(r.body))\n",
    "  append!(lats,parse(Float64,bod[1][\"lat\"]))\n",
    "   append!(lons,parse(Float64,bod[1][\"lon\"]))\n",
    "  sleep(1)\n",
    "  catch\n",
    "    append!(lats,NaN)\n",
    "   append!(lons,NaN)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We want to append the Latitude & Longitude values on to our DataFrame and then remove any NaN values (there was only 1 when I searched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-24T15:13:18.462Z",
     "iopub.status.busy": "2020-06-24T15:13:18.440Z",
     "iopub.status.idle": "2020-06-24T15:13:18.495Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_adds[!,:LATITUDE] = lats;\n",
    "D_adds[!,:LONGITUDE] = lons;\n",
    "D_adds = D_adds[findall(.!(isnan.(D_adds[!,:LATITUDE]))),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:31:01.183Z",
     "iopub.status.busy": "2020-06-25T16:30:59.690Z",
     "iopub.status.idle": "2020-06-25T16:32:49.751Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames, Gadfly, Queryverse, Statistics, Dates,Plots,StatsPlots, VegaDatasets,CSV, JSON\n",
    "plotly()\n",
    "Plots.PlotlyBackend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Let's take a look at the values on a map, to ensure that the returned values were correct. To do this we'll import the ```us-10m``` dataset from ```VegaDatasets``` which contains the boundaries of the US states at the 1:10,000,000 scale. The excellent [Vega Projection Editor](https://vega.github.io/vega/docs/projections/) allowed me to customise the projection settings to ensure that the map was centred on Manhattan, but not too zoomed in to miss out any outlying data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:42.160Z",
     "iopub.status.busy": "2020-06-25T16:36:42.136Z",
     "iopub.status.idle": "2020-06-25T16:36:49.079Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "us10m = dataset(\"us-10m\")\n",
    "@vlplot(width=800, height=500) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=\"#eee\",\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:azimuthalEqualArea,scale=2500,center=[29,43],rotate=[105,0,0]},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=D_adds,\n",
    "    projection={type=:azimuthalEqualArea,scale=2500,center=[29,43],rotate=[105,0,0]},\n",
    "    longitude={\"LONGITUDE:q\"},\n",
    "   latitude={\"LATITUDE:q\"},\n",
    "   size={value=5},\n",
    "   color={value=:blue}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We can see a few incorrect data points, some are in upstate New York and some others are in Brooklyn. Hovering over some of the points with the mouse brings up the Lat/Lon values. We can see that those points with Longitude > -73.7 are on Brooklyn and Latitude > 41 are in upstate New York. Let's zoom in further and take a deeper look at Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:49.139Z",
     "iopub.status.busy": "2020-06-25T16:36:49.113Z",
     "iopub.status.idle": "2020-06-25T16:36:51.026Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "us10m = dataset(\"us-10m\")\n",
    "@vlplot(width=800, height=500) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=\"#eee\",\n",
    "        stroke=:white\n",
    "   },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:azimuthalEqualArea,scale=15000,center=[29,40.5],rotate=[102.5,0,0]},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=D_adds,\n",
    "    projection={type=:azimuthalEqualArea,scale=15000,center=[29,40.5],rotate=[102.5,0,0]},\n",
    "    longitude={\"LONGITUDE:q\"},\n",
    "    latitude={\"LATITUDE:q\"},\n",
    "    size={value=5},\n",
    "    color={value=:blue}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Zooming in allows us to identify extra mislocated properties. To separate out the incorrect locations I downloaded the boundary of New York City Boroughs from [NYC OpenData](https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm). It is important to note here that the boundary is defined by 34 separate polygons as Manhattan includes a number of islands (e.g. the Statue of Liberty is on Liberty Island which is included as a separate polygon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:51.081Z",
     "iopub.status.busy": "2020-06-25T16:36:51.059Z",
     "iopub.status.idle": "2020-06-25T16:36:51.143Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using JSON\n",
    "borofile = open(\"BoroughBoundaries.geojson\")\n",
    "headers = JSON.parse(read(borofile,String))\n",
    "println(headers[\"features\"][3][\"properties\"])\n",
    "manh_boundary = headers[\"features\"][3][\"geometry\"][\"coordinates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "I then created a new column in our DataFrame that will be true if the location is within Manhattan and false if it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:51.191Z",
     "iopub.status.busy": "2020-06-25T16:36:51.172Z",
     "iopub.status.idle": "2020-06-25T16:36:51.222Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_adds[!,:MANHATTAN] = falses(1449);\n",
    "nrs = size(D_adds)[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Using the [Luxor](https://github.com/JuliaGraphics/Luxor.jl) Package we can iterate through all of the boundary polygons and check whether each location is within the boundary of Manhattan. We'll then fill the relevant values in the D_adds DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:51.270Z",
     "iopub.status.busy": "2020-06-25T16:36:51.251Z",
     "iopub.status.idle": "2020-06-25T16:36:52.055Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using Luxor\n",
    "using ProgressMeter\n",
    "@showprogress for region in manh_boundary\n",
    "  bnd = [Point(p[1],p[2]) for p in region[1]];\n",
    "  for i in range(1,stop=nrs)\n",
    "    coord = Point(D_adds[i,:LONGITUDE],D_adds[i,:LATITUDE]);\n",
    "    isin = isinside(coord,bnd);\n",
    "    if (isin==true)\n",
    "      D_adds[i,:MANHATTAN] = true\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Lets filter out the incorrect locations and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:53.570Z",
     "iopub.status.busy": "2020-06-25T16:36:53.549Z",
     "iopub.status.idle": "2020-06-25T16:36:56.408Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "D_adds = D_adds |> @filter(_.MANHATTAN >0) |> DataFrame;\n",
    "@vlplot(width=800, height=500) +\n",
    "@vlplot(\n",
    "    mark={\n",
    "        :geoshape,\n",
    "        fill=\"#eee\",\n",
    "        stroke=:white\n",
    "    },\n",
    "    data={\n",
    "        values=us10m,\n",
    "        format={\n",
    "            type=:topojson,\n",
    "            feature=:states\n",
    "        }\n",
    "    },\n",
    "    projection={type=:azimuthalEqualArea,scale=15000,center=[29,40.5],rotate=[102.5,0,0]},\n",
    ") +\n",
    "@vlplot(\n",
    "    :circle,\n",
    "    data=D_adds,\n",
    "    projection={type=:azimuthalEqualArea,scale=15000,center=[29,40.5],rotate=[102.5,0,0]},\n",
    "    longitude={\"LONGITUDE:q\"},\n",
    "    latitude={\"LATITUDE:q\"},\n",
    "    size={value=5},\n",
    "    color={value=:blue}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:56.454Z",
     "iopub.status.busy": "2020-06-25T16:36:56.435Z",
     "iopub.status.idle": "2020-06-25T16:36:56.489Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "println(size(D_adds)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We still have 1339 properties to use for our classification, which is a good amount. Let's prepare the data ready for passing into MLJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:36:59.097Z",
     "iopub.status.busy": "2020-06-25T16:36:59.076Z",
     "iopub.status.idle": "2020-06-25T16:37:03.692Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "using MLJ,MLJLinearModels\n",
    "D_adds_sub = D_adds |> @map({X = _.LONGITUDE,X1 = _.LATITUDE,Y = _.NEIGHBORHOOD}) |> DataFrame \n",
    "y,X = unpack(D_adds_sub,==(:Y),!=(:Y));\n",
    "y=categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We can search for the KNN model using the following command, handy if we don't know the exact name of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:35:35.162Z",
     "iopub.status.busy": "2020-06-25T16:35:32.473Z",
     "iopub.status.idle": "2020-06-25T16:35:38.903Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "models(\"Neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Let's set up a KNN and increase the number of neighbours to use for the classification. We should use Cross Validation here, however I'm omitting for this simple example. I also had a few issues in testing where the CV was not selecting at least 1 of each type in the training sets the evaluate function was giving out an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:37:04.296Z",
     "iopub.status.busy": "2020-06-25T16:37:04.278Z",
     "iopub.status.idle": "2020-06-25T16:37:20.084Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "function knn_acc(yhat,y)\n",
    "  return sum(yhat .== y)/length(y)\n",
    "end\n",
    "nbrs = 1:10\n",
    "accs = []\n",
    "@load KNeighborsClassifier pkg=\"ScikitLearn\"\n",
    "for k in nbrs\n",
    "model = KNeighborsClassifier(n_neighbors = k);\n",
    "knn = machine(model, X, y);\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split\n",
    "fit!(knn, rows=train);\n",
    "yhat = predict_mode(knn, rows=test);\n",
    "append!(accs,knn_acc(yhat,y[test]));\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Finally, let's plot the accuracy at each value of k. As we can see a k = 1 provides the highest accuracy in this case, which may be somewhat surprising, however this is an overly simplified example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-25T16:38:10.017Z",
     "iopub.status.busy": "2020-06-25T16:38:09.997Z",
     "iopub.status.idle": "2020-06-25T16:38:10.104Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Plots.plot(nbrs,accs,seriestype=:scatter,xlabel=\"n_neighbours\", ylabel = \"Accuracy\",size = (300,300),ylim = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "julia-1.4"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "nteract": {
   "version": "0.23.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
